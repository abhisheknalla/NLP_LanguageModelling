{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZgGNXZEfywz"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deLPHwbIX3-r"
   },
   "outputs": [],
   "source": [
    "# Download gutenberg dataset\n",
    "# This is the dataset on which all your models will be trained\n",
    "# https://drive.google.com/file/d/0B2Mzhc7popBga2RkcWZNcjlRTGM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "\n",
    "puncts = \"('|;|:|-,!)\"\n",
    "caps = \"([A-Z])\"\n",
    "smalls =  \"([a-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "corpus = []\n",
    "corpus2 = []\n",
    "corpus3 = []\n",
    "def tokenize(path,type):\n",
    "    #corpus2=[]\n",
    "    if type==1:\n",
    "        files = glob.glob(path)\n",
    "        #print(files[0])\n",
    "        # (a)\\\\1 is back reference--> looks if a occurs again\n",
    "        for name in files:\n",
    "            #print(name)\n",
    "            try:\n",
    "                with open(name,'r',errors = 'replace') as f:\n",
    "                    text = f.read()\n",
    "                    #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "                    text = \" \" + text + \"  \"\n",
    "        #             text = text.replace(\",\",\"  ,\")\n",
    "                    text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "                    text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "                    text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "                    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "                    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "                    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "                    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "                    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "                    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "                    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "                    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "\n",
    "                    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "                    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "                    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "                    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "                    text = text.replace(\".\",\".<stop>\")\n",
    "                    text = text.replace(\"?\",\"?<stop>\")\n",
    "                    text = text.replace(\"!\",\"!<stop>\")\n",
    "                    text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "\n",
    "                    text = text.replace(\"--\",\" \" + \"-\" + \"-\" + \" \")\n",
    "                    text = text.replace('(',' ( ')\n",
    "                    text = text.replace(')',' ) ')\n",
    "                    text = text.replace(\",\",\" ,\")\n",
    "                    text = text.replace('\"',' \" ')\n",
    "                    text = text.replace(\"?\",\" ?\")\n",
    "                    text = text.replace(\".<stop>\",\" .<stop>\")\n",
    "                    text = text.replace(\"?<stop>\",\" ?<stop>\")\n",
    "                    text = text.replace(\"!<stop>\",\" !<stop>\")\n",
    "                    text = re.sub(puncts + \" \",\" \\\\1 \",text)\n",
    "                    text = re.sub(\" \" + puncts,\" \\\\1 \",text)\n",
    "                    corpus2.append(text.split())\n",
    "                    f.close()\n",
    "\n",
    "            except IOError as exc:\n",
    "                if exc.errno != errno.EISDIR:\n",
    "                    raise\n",
    "    if type==2:\n",
    "        corpus3=[]\n",
    "        text = path\n",
    "        #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "        text = \" \" + text + \"  \"\n",
    "#             text = text.replace(\",\",\"  ,\")\n",
    "        text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "        text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "        text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "        if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "        text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "        text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "        text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "        text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "\n",
    "        if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "        if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "        if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "        if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        text = text.replace(\".\",\".<stop>\")\n",
    "        text = text.replace(\"?\",\"?<stop>\")\n",
    "        text = text.replace(\"!\",\"!<stop>\")\n",
    "        text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "\n",
    "        text = text.replace(\"--\",\" \" + \"-\" + \"-\" + \" \")\n",
    "        text = text.replace('(',' ( ')\n",
    "        text = text.replace(')',' ) ')\n",
    "        text = text.replace(\",\",\" ,\")\n",
    "        text = text.replace('\"',' \" ')\n",
    "        text = text.replace(\"?\",\" ?\")\n",
    "        text = text.replace(\".<stop>\",\" .<stop>\")\n",
    "        text = text.replace(\"?<stop>\",\" ?<stop>\")\n",
    "        text = text.replace(\"!<stop>\",\" !<stop>\")\n",
    "        text = re.sub(puncts + \" \",\" \\\\1 \",text)\n",
    "        text = re.sub(\" \" + puncts,\" \\\\1 \",text)\n",
    "        corpus3.append(text.split())\n",
    "        return corpus3\n",
    "        \n",
    "    print(corpus2[0])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-56-ea6db75afbab>\", line 1, in <module>\n",
      "    os.chdir(r'~/5thsem/NLP/assignments/assignment1')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '~/5thsem/NLP/assignments/assignment1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/abhi/2k18Summer/UdemyML/ENTER/lib/python3.6/posixpath.py\", line 376, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/5thsem/NLP/assignments/assignment1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "os.chdir(r'~/5thsem/NLP/assignments/assignment1')\n",
    "#path = './a.txt'\n",
    "tokenize('./Gutenberg/txt/Ab*.txt',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08q0ospof2z9"
   },
   "outputs": [],
   "source": [
    "def sentence_tokenize(path):\n",
    "    files = glob.glob(path)\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name,'r',errors = 'replace') as f:\n",
    "                text = f.read()\n",
    "                #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "                text = \" \" + text + \"  \"\n",
    "                text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "                text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "                text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "                if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "                text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "                text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "                text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "                text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "                text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "                text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "                text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "                \n",
    "                if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "                if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "                if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "                if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "                text = text.replace(\".\",\".<stop>\")\n",
    "                text = text.replace(\"?\",\"?<stop>\")\n",
    "                text = text.replace(\"!\",\"!<stop>\")\n",
    "                text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "                sentences = text.split(\"<stop>\")\n",
    "                sentences = sentences[:-1]\n",
    "                sentences = [s.strip() for s in sentences]\n",
    "                corpus.append(sentences)\n",
    "\n",
    "                f.close()\n",
    "\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "    #print(corpus[0])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Gutenberg/txt/Ab*.txt'\n",
    "sentence_tokenize(path)\n",
    "\n",
    "# sentence = \"I like you\"\n",
    "# sentence_tokenize(sentence,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gwBEe5bgAbn"
   },
   "source": [
    "# Language Model and Smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input):#flattening a 2d list  to a 1d list\n",
    "    words = []\n",
    "    for i in input:\n",
    "        for j in i:\n",
    "            words.append(j)\n",
    "    return words\n",
    "\n",
    "words = flatten(corpus2)\n",
    "#print(words[-2])\n",
    "\n",
    "from collections import Counter # counting frequencies of words\n",
    "exclusion_list = [\",\", \".<stop>\", '\"','-',':',';',\"'\"]\n",
    "Counter = Counter(word.rstrip() for word in words if word not in exclusion_list)\n",
    "most_occur = []\n",
    "most_occur = Counter.most_common(100)\n",
    "print(most_occur)\n",
    "a = ('word','frequency')\n",
    "most_occur_table = [a] + most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as FF\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "data = most_occur_table\n",
    "df = data[0:11]\n",
    "\n",
    "table = FF.create_table(df) # making a table\n",
    "#py.sign_in('abhinalla','••••••••••')\n",
    "iplot(table, filename='word-count-sample')\n",
    "\n",
    "data = most_occur\n",
    "iplot([{\"x\" : list(zip(*data))[0], \"y\": list(zip(*data))[1]}])# plotting of frequencies against words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------------ method 2\n",
    "# Unigram\n",
    "import math\n",
    "\n",
    "def token_counts(corpus):\n",
    "    tokens = {}\n",
    "    #for file in corp:\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            if word.lower() not in exclusion_list:\n",
    "                if word.lower() not in tokens:\n",
    "                    tokens[word.lower()] = 0\n",
    "                tokens[word.lower()] += 1\n",
    "    return tokens\n",
    "# print(corpus2[0])\n",
    "from ipy_table import *\n",
    "unigrams = token_counts(corpus2)\n",
    "#print(unigrams)\n",
    "sorted_unigrams = sorted(unigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "display(make_table(sorted_unigrams[:10]))\n",
    "\n",
    "sorted_unigrams2 = sorted(unigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "display(make_table(sorted_unigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : list(zip(*sorted_unigrams[:1000]))[0], \"y\": list(zip(*sorted_unigrams[:1000]))[1]}])\n",
    "\n",
    "x = list(zip(*sorted_unigrams[:1000]))[0]\n",
    "y = list(zip(*sorted_unigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "#----------------------------------\n",
    "all_sort_uni = sorted_unigrams[:len(unigrams)]\n",
    "\n",
    "freq_class_uni = {}\n",
    "\n",
    "for each in all_sort_uni:\n",
    "#     print(each[1])\n",
    "    if each[1] not in freq_class_uni:\n",
    "        freq_class_uni[each[1]] = 0\n",
    "    freq_class_uni[each[1]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram ------------------\n",
    "def get_bigrams(corpus):\n",
    "    bigrams = {}\n",
    "    for sentence in corpus:\n",
    "        for index, word in enumerate(sentence):\n",
    "            if index > 0:\n",
    "                pair = (sentence[index - 1].lower(), word.lower())\n",
    "                if word.lower() not in exclusion_list and sentence[index-1].lower() not in exclusion_list:\n",
    "                    if pair not in bigrams:\n",
    "                        bigrams[pair] =0\n",
    "                    bigrams[pair] += 1        \n",
    "    return bigrams\n",
    "\n",
    "# Note :: Try this with the smaller corpus first.\n",
    "bigrams = get_bigrams(corpus2)\n",
    "sorted_bigrams = sorted(bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "sorted_bigrams2 = sorted(bigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "\n",
    "sorted_bigrams = sorted_bigrams[:1000] # RAM problems\n",
    "print(sorted_bigrams[:100]) \n",
    "display(make_table(sorted_bigrams[:10]))\n",
    "display(make_table(sorted_bigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : ['_'.join(x) for x in list(zip(*sorted_bigrams))[0]], \"y\": list(zip(*sorted_bigrams))[1]}])\n",
    "\n",
    "x = ['_'.join(x) for x in list(zip(*sorted_bigrams))[0]]\n",
    "y = list(zip(*sorted_bigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "\n",
    "# ----------------------------------------------\\\n",
    "\n",
    "sorted_bigrams = sorted(bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "all_sort = sorted_bigrams[:len(bigrams)]\n",
    "\n",
    "freq_class_bi = {}\n",
    "\n",
    "for each in all_sort:\n",
    "#     print(each[1])\n",
    "    if each[1] not in freq_class_bi:\n",
    "        freq_class_bi[each[1]] = 0\n",
    "    freq_class_bi[each[1]] += 1\n",
    "#     print(freq_class[each[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram ------------------\n",
    "def get_trigrams(corpus):\n",
    "    trigrams = {}\n",
    "    for sentence in corpus:\n",
    "        for index, word in enumerate(sentence):\n",
    "            if index > 1:\n",
    "                trio = (sentence[index-2].lower(), sentence[index - 1].lower(), word.lower())\n",
    "                if word.lower() not in exclusion_list and sentence[index-2].lower() not in exclusion_list and sentence[index-1].lower() not in exclusion_list:\n",
    "                    if trio not in trigrams:\n",
    "                        trigrams[trio] =0\n",
    "                    trigrams[trio] += 1        \n",
    "    return trigrams\n",
    "\n",
    "# Note :: Try this with the smaller corpus first.\n",
    "trigrams = get_trigrams(corpus2)\n",
    "sorted_trigrams = sorted(trigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "sorted_trigrams2 = sorted(trigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "\n",
    "sorted_trigrams = sorted_trigrams[:100] # RAM problems\n",
    "print(sorted_trigrams) \n",
    "display(make_table(sorted_trigrams[:10]))\n",
    "display(make_table(sorted_trigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : ['_'.join(x) for x in list(zip(*sorted_trigrams))[0]], \"y\": list(zip(*sorted_trigrams))[1]}])\n",
    "\n",
    "x = ['_'.join(x) for x in list(zip(*sorted_trigrams))[0]]\n",
    "y = list(zip(*sorted_trigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "\n",
    "#----------------------------------------\n",
    "sorted_trigrams = sorted(trigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "all_sort = sorted_trigrams[:len(trigrams)]\n",
    "\n",
    "freq_class_tri = {}\n",
    "\n",
    "for each in all_sort:\n",
    "    #print(each[1])\n",
    "    if each[1] not in freq_class_tri:\n",
    "        freq_class_tri[each[1]] = 0\n",
    "    freq_class_tri[each[1]] += 1\n",
    "\n",
    "#graphs wont be smooth because we are taking only 100 tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d, reverse = True):\n",
    "    return sorted(d.items(), key = lambda x : x[1], reverse = reverse)\n",
    "\n",
    "def unigram_probs(unigrams):\n",
    "    new_unigrams = {}\n",
    "    N = sum(unigrams.values())\n",
    "    for word in unigrams:\n",
    "        new_unigrams[word] = round(unigrams[word] / float(N), 15)\n",
    " #new_unigrams[word] = math.log(unigrams[word] / float(N))\n",
    "    return new_unigrams\n",
    " \n",
    "uprobs = unigram_probs(unigrams)\n",
    "sorted_uprobs = sort_dict(uprobs)\n",
    "make_table(sorted_uprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probs(unigrams):\n",
    "    new_bigrams = {}\n",
    "    N = sum(bigrams.values())\n",
    "    for word in bigrams:\n",
    "        new_bigrams[word] = round(bigrams[word] / float(N), 15)\n",
    " #new_unigrams[word] = math.log(unigrams[word] / float(N))\n",
    "    return new_bigrams\n",
    " \n",
    "bprobs = bigram_probs(bigrams)\n",
    "sorted_bprobs = sort_dict(bprobs)\n",
    "make_table(sorted_bprobs[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N from (N-1) gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Th9ogrogKTO"
   },
   "outputs": [],
   "source": [
    "#print(corpus2)\n",
    "def complete_ngram(ngram, n):\n",
    "    comp_ngram = {}\n",
    "    for each in corpus2:\n",
    "        for index, word in enumerate(each):\n",
    "            if word not in exclusion_list:\n",
    "                if index + n <= len(each) - 1:\n",
    "                    flag = 0\n",
    "                    for i in range(n):                        \n",
    "                        if ngram[i].lower() != each[index+i].lower():\n",
    "                            flag=1\n",
    "                            break\n",
    "\n",
    "                    if flag == 0:\n",
    "                        list_ng = list(ngram)\n",
    "                        if each[index+n-1].lower() not in exclusion_list:\n",
    "                            list_ng.append(each[index+n].lower())              \n",
    "                            ng_tuple = tuple(list_ng)\n",
    "        #                     list_ngram = list(ngram)\n",
    "        #                     list_ngram.append(each[index+n-1].lower())\n",
    "                            if ng_tuple not in comp_ngram:\n",
    "                                    comp_ngram[ng_tuple] =0\n",
    "                            #print('HERE')\n",
    "                            #print(ng_tuple)\n",
    "                    \n",
    "                            comp_ngram[ng_tuple] += 1\n",
    "                            #print(comp_ngram[ng_tuple])\n",
    "    return sort_dict(comp_ngram)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ngram(('the','united'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def laplace_smoothing(n_grams):\n",
    "    n = len(n_grams)\n",
    " #   print(n)\n",
    "    if n==1:\n",
    "  #      print(n_grams)     \n",
    "        if n_grams not in unigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = unigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(unigrams.values()) + n )* 1.0)  \n",
    "    if n == 2:\n",
    "        if n_grams not in bigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = bigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(bigrams.values()) + n*(n-1)) * 1.0)\n",
    "    if n == 3:\n",
    "        if n not in trigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = trigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(bigrams.values()) + n*(n-1)*(n-2)) * 1.0)\n",
    "    return ans_value\n",
    "\n",
    "\n",
    "def good_turing(n_grams):\n",
    "    \n",
    "    if len(list(n_grams)) == 3:\n",
    "        if n_grams in trigrams:\n",
    "#             trigrams[word] + 1\n",
    "            if trigrams[n_grams] + 1 in freq_class_tri:\n",
    "                num = (trigrams[n_grams] + 1) * freq_class_tri[trigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_tri[trigrams[n_grams]]\n",
    "            prob = (num/den)/sum(trigrams.values())\n",
    "        else: \n",
    "            count = 1\n",
    "            if count in freq_class_tri:\n",
    "                num = (trigrams[n_grams] + 1) * freq_class_tri[trigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_tri[trigrams[n_grams]]\n",
    "            prob = num/den\n",
    "            \n",
    "    if len(list(n_grams)) == 2:\n",
    "        if n_grams in bigrams:\n",
    "#             trigrams[word] + 1\n",
    "            if bigrams[n_grams] + 1 in freq_class_bi:\n",
    "                num = (bigrams[n_grams] + 1) * freq_class_bi[bigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            print(bigrams[n_grams])\n",
    "            print(freq_class_bi[bigrams[n_grams]])\n",
    "            den = freq_class_bi[bigrams[n_grams]]\n",
    "            \n",
    "            prob = (num/den)/sum(freq_class_bi.values())\n",
    "        else: \n",
    "            count = 1\n",
    "            if count in freq_class_bi:\n",
    "                num = (bigrams[n_grams] + 1) * freq_class_bi[bigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_bi[bigrams[n_grams]]\n",
    "            prob = num/den\n",
    "            \n",
    "    return prob\n",
    "\n",
    "def good_turing_uni(n_grams):\n",
    "#     print('HERE')\n",
    "#     print(unigrams)\n",
    "    if n_grams in unigrams:\n",
    "        if unigrams[n_grams] + 1 in freq_class_uni:\n",
    "            prob = ((unigrams[n_grams]+ 1) * freq_class_uni[unigrams[n_grams]] / (freq_class_uni[unigrams[n_grams] ] * 1.0)) / (freq_class_uni[1] * 1.0)\n",
    "        else:\n",
    "            prob = 0\n",
    "    else:\n",
    "        if 1 in freq_class_uni:\n",
    "            prob = ((unigrams[n_grams] + 1) * freq_class_uni[unigrams[n_grams]] / ( sum(unigrams.values()) * 1.0))\n",
    "        else:\n",
    "            prob = 0\n",
    "\n",
    "    return prob\n",
    "\n",
    "def deleted_interpolation(n_grams):\n",
    "  # perform deleted Interpolation\n",
    "#     list_grams = list(n_grams)\n",
    "    n = len(n_grams)\n",
    "    if n == 1:\n",
    "        return laplace_smoothing(n_grams)\n",
    "    if n == 2:\n",
    "        return 0.7*laplace_smoothing(n_grams) + 0.3*laplace_smoothing((n_grams[1],))\n",
    "    else:\n",
    "        return ((0.5)*laplace_smoothing(n_grams) + (0.4)*laplace_smoothing(n_grams[1:3]) + (0.1)*laplace_smoothing((n_grams[2],)))\n",
    "\n",
    "\n",
    "def kneser_ney(n_grams):\n",
    "  # perform Kneser-Ney smoothing\n",
    "    pass\n",
    "\n",
    "\n",
    "# only 1 of the next 2 have to be implemented\n",
    "\n",
    "def backoff(n_grams):\n",
    "  # perform Backoff\n",
    "    pass\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams[('as','soon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth1 = ['of','the']\n",
    "\n",
    "#laplace_smoothing((smooth1[0],smooth1[1]))\n",
    "\n",
    "# good_turing(('as','soon'))\n",
    "#good_turing_uni(('a'))\n",
    "\n",
    "# deleted_interpolation(('the','united','states'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjoF5Wlsaf_M"
   },
   "source": [
    "# Unigrams and Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Check and Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oehobp6dbXp7"
   },
   "outputs": [],
   "source": [
    "alphas = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "def spell_checker(check_word, n_grams):\n",
    "    list_correct = []\n",
    "    max_correct = ''\n",
    "    max_count=-1\n",
    "    word_count = 0\n",
    "    for each_alpha in alphas:\n",
    "        for i in range(len(list(check_word))):\n",
    "#             print(i)\n",
    "            list_word = list(check_word)\n",
    "            list_word[i] = each_alpha\n",
    "#             new_ word = check_word.replace(check_word[i],each_alpha)\n",
    "            if (\"\".join(list_word)) in n_grams:\n",
    "                new_word = \"\".join(list_word)\n",
    "                list_correct.append([new_word,unigrams[new_word]])\n",
    "                if unigrams[new_word] > max_count:\n",
    "                    max_count = unigrams[new_word]\n",
    "                    max_correct = new_word\n",
    "    for each_alpha in alphas:\n",
    "        for i in range(len(check_word) - 1):\n",
    "            list_word = list(check_word)\n",
    "            list_word[i] = each_alpha\n",
    "#             new_word = check_word[i].replace(check_word[i],each_alpha)\n",
    "            for each_next_alpha in alphas:\n",
    "                list_word[i+1] = each_next_alpha\n",
    "#                 new_word = new_word[i].replace(new_word[i+1],each_next_alpha)\n",
    "                if (\"\".join(list_word)) in n_grams:\n",
    "                    new_word = \"\".join(list_word)\n",
    "                    list_correct.append([new_word,unigrams[new_word]])\n",
    "                    if unigrams[new_word] > max_count:\n",
    "                        max_count = unigrams[new_word]\n",
    "                        max_correct = new_word\n",
    "                    \n",
    "    return max_correct, max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker('spalling',unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgPpfeyYgLUv"
   },
   "source": [
    "# Grammaticality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX0RvOpLgYgR"
   },
   "outputs": [],
   "source": [
    "def score_grammaticality(sentence):\n",
    "    words = []\n",
    "    value = 1   \n",
    "    print(sentence)\n",
    "    arr = tokenize(sentence,2)\n",
    "    print(arr)\n",
    "    #re.split(r':|;|,|-| ',sentence)\n",
    "    if arr:       \n",
    "        for each in arr:\n",
    "            print(each)\n",
    "            for word in each:\n",
    "                \n",
    "                word = word.strip(\"'|)|?|!|.|,|(\")\n",
    "                word = word.strip('\"')\n",
    "                #print(word)\n",
    "                if word:\n",
    "                    words.append((word.lower()))\n",
    "                #print(len(words))\n",
    "                if len(words) == 1:\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "                elif len(words) == 2:\n",
    "                    value = value*laplace_smoothing((words[0],words[1]))\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "                elif len(words)>2:\n",
    "                    for j in range(len(words)-2):\n",
    "                        value = value*(laplace_smoothing((words[j],words[j+1],words[j+2])))\n",
    "                    value = value*(laplace_smoothing((words[0],words[1])))\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = score_grammaticality('I have a red apple.')\n",
    "print(x)\n",
    "\n",
    "g = score_grammaticality('apple a have I red.')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Error Correction Demo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
