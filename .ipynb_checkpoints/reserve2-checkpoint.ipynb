{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZgGNXZEfywz"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deLPHwbIX3-r"
   },
   "outputs": [],
   "source": [
    "# Download gutenberg dataset\n",
    "# This is the dataset on which all your models will be trained\n",
    "# https://drive.google.com/file/d/0B2Mzhc7popBga2RkcWZNcjlRTGM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "\n",
    "puncts = \"('|;|:|-,!)\"\n",
    "caps = \"([A-Z])\"\n",
    "smalls =  \"([a-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "corpus = []\n",
    "corpus2 = []\n",
    "corpus3 = []\n",
    "def tokenize(path,type):\n",
    "    #corpus2=[]\n",
    "    if type==1:\n",
    "        files = glob.glob(path)\n",
    "        #print(files[0])\n",
    "        # (a)\\\\1 is back reference--> looks if a occurs again\n",
    "        for name in files:\n",
    "            #print(name)\n",
    "            try:\n",
    "                with open(name,'r',errors = 'replace') as f:\n",
    "                    text = f.read()\n",
    "                    #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "                    text = \" \" + text + \"  \"\n",
    "        #             text = text.replace(\",\",\"  ,\")\n",
    "                    text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "                    text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "                    text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "                    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "                    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "                    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "                    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "                    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "                    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "                    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "                    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "\n",
    "                    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "                    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "                    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "                    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "                    text = text.replace(\".\",\".<stop>\")\n",
    "                    text = text.replace(\"?\",\"?<stop>\")\n",
    "                    text = text.replace(\"!\",\"!<stop>\")\n",
    "                    text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "\n",
    "                    text = text.replace(\"--\",\" \" + \"-\" + \"-\" + \" \")\n",
    "                    text = text.replace('(',' ( ')\n",
    "                    text = text.replace(')',' ) ')\n",
    "                    text = text.replace(\",\",\" ,\")\n",
    "                    text = text.replace('\"',' \" ')\n",
    "                    text = text.replace(\"?\",\" ?\")\n",
    "                    text = text.replace(\".<stop>\",\" .<stop>\")\n",
    "                    text = text.replace(\"?<stop>\",\" ?<stop>\")\n",
    "                    text = text.replace(\"!<stop>\",\" !<stop>\")\n",
    "                    text = re.sub(puncts + \" \",\" \\\\1 \",text)\n",
    "                    text = re.sub(\" \" + puncts,\" \\\\1 \",text)\n",
    "                    corpus2.append(text.split())\n",
    "                    f.close()\n",
    "\n",
    "            except IOError as exc:\n",
    "                if exc.errno != errno.EISDIR:\n",
    "                    raise\n",
    "    if type==2:\n",
    "        corpus3=[]\n",
    "        text = path\n",
    "        #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "        text = \" \" + text + \"  \"\n",
    "#             text = text.replace(\",\",\"  ,\")\n",
    "        text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "        text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "        text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "        if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "        text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "        text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "        text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "        text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "\n",
    "        if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "        if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "        if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "        if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        text = text.replace(\".\",\".<stop>\")\n",
    "        text = text.replace(\"?\",\"?<stop>\")\n",
    "        text = text.replace(\"!\",\"!<stop>\")\n",
    "        text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "\n",
    "        text = text.replace(\"--\",\" \" + \"-\" + \"-\" + \" \")\n",
    "        text = text.replace('(',' ( ')\n",
    "        text = text.replace(')',' ) ')\n",
    "        text = text.replace(\",\",\" ,\")\n",
    "        text = text.replace('\"',' \" ')\n",
    "        text = text.replace(\"?\",\" ?\")\n",
    "        text = text.replace(\".<stop>\",\" .<stop>\")\n",
    "        text = text.replace(\"?<stop>\",\" ?<stop>\")\n",
    "        text = text.replace(\"!<stop>\",\" !<stop>\")\n",
    "        text = re.sub(puncts + \" \",\" \\\\1 \",text)\n",
    "        text = re.sub(\" \" + puncts,\" \\\\1 \",text)\n",
    "        corpus3.append(text.split())\n",
    "        return corpus3\n",
    "        \n",
    "    print(corpus2[0])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Lincoln's\", 'Second', 'Inaugural', 'Address', 'March', '4', ',', '1865', 'Fellow', 'countrymen', ':', 'At', 'this', 'second', 'appearing', 'to', 'take', 'the', 'oath', 'of', 'the', 'presidential', 'office', ',', 'there', 'is', 'less', 'occasion', 'for', 'an', 'extended', 'address', 'than', 'there', 'was', 'at', 'the', 'first', '.<stop>', 'Then', 'a', 'statement', ',', 'somewhat', 'in', 'detail', ',', 'of', 'a', 'course', 'to', 'be', 'pursued', ',', 'seemed', 'fitting', 'and', 'proper', '.<stop>', 'Now', ',', 'at', 'the', 'expiration', 'of', 'four', 'years', ',', 'during', 'which', 'public', 'declarations', 'have', 'been', 'constantly', 'called', 'forth', 'on', 'every', 'point', 'and', 'phase', 'of', 'the', 'great', 'contest', 'which', 'still', 'absorbs', 'the', 'attention', 'and', 'engrosses', 'the', 'energies', 'of', 'the', 'nation', ',', 'little', 'that', 'is', 'new', 'could', 'be', 'presented', '.<stop>', 'The', 'progress', 'of', 'our', 'arms', ',', 'upon', 'which', 'all', 'else', 'chiefly', 'depends', ',', 'is', 'as', 'well', 'known', 'to', 'the', 'public', 'as', 'to', 'myself', ';', 'and', 'it', 'is', ',', 'I', 'trust', ',', 'reasonably', 'satisfactory', 'and', 'encouraging', 'to', 'all', '.<stop>', 'With', 'high', 'hope', 'for', 'the', 'future', ',', 'no', 'prediction', 'in', 'regard', 'to', 'it', 'is', 'ventured', '.<stop>', 'On', 'the', 'occasion', 'corresponding', 'to', 'this', 'four', 'years', 'ago', ',', 'all', 'thoughts', 'were', 'anxiously', 'directed', 'to', 'an', 'impending', 'civil', 'war', '.<stop>', 'All', 'dreaded', 'it', '--', 'all', 'sought', 'to', 'avert', 'it', '.<stop>', 'While', 'the', 'inaugural', 'address', 'was', 'being', 'delivered', 'from', 'this', 'place', ',', 'devoted', 'altogether', 'to', 'saving', 'the', 'Union', 'without', 'war', ',', 'insurgent', 'agents', 'were', 'in', 'the', 'city', 'seeking', 'to', 'destroy', 'it', 'without', 'war', '--', 'seeking', 'to', 'dissolve', 'the', 'Union', ',', 'and', 'divide', 'effects', ',', 'by', 'negotiation', '.<stop>', 'Both', 'parties', 'deprecated', 'war', ';', 'but', 'one', 'of', 'them', 'would', 'make', 'war', 'rather', 'than', 'let', 'the', 'nation', 'survive', ';', 'and', 'the', 'other', 'would', 'accept', 'war', 'rather', 'than', 'let', 'it', 'perish', '.<stop>', 'And', 'the', 'war', 'came', '.<stop>', 'One-eighth', 'of', 'the', 'whole', 'population', 'were', 'colored', 'slaves', ',', 'not', 'distributed', 'generally', 'over', 'the', 'Union', ',', 'but', 'localized', 'in', 'the', 'Southern', 'part', 'of', 'it', '.<stop>', 'These', 'slaves', 'constituted', 'a', 'peculiar', 'and', 'powerful', 'interest', '.<stop>', 'All', 'knew', 'that', 'this', 'interest', 'was', ',', 'somehow', ',', 'the', 'cause', 'of', 'the', 'war', '.<stop>', 'To', 'strengthen', ',', 'perpetuate', ',', 'and', 'extend', 'this', 'interest', 'was', 'the', 'object', 'for', 'which', 'the', 'insurgents', 'would', 'rend', 'the', 'Union', ',', 'even', 'by', 'war', ';', 'while', 'the', 'government', 'claimed', 'no', 'right', 'to', 'do', 'more', 'than', 'to', 'restrict', 'the', 'territorial', 'enlargement', 'of', 'it', '.<stop>', 'Neither', 'party', 'expected', 'for', 'the', 'war', 'the', 'magnitude', 'or', 'the', 'duration', 'which', 'it', 'has', 'already', 'attained', '.<stop>', 'Neither', 'anticipated', 'that', 'the', 'cause', 'of', 'the', 'conflict', 'might', 'cease', 'with', ',', 'or', 'even', 'before', ',', 'the', 'conflict', 'itself', 'should', 'cease', '.<stop>', 'Each', 'looked', 'for', 'an', 'easier', 'triumph', ',', 'and', 'a', 'result', 'less', 'fundamental', 'and', 'astounding', '.<stop>', 'Both', 'read', 'the', 'same', 'Bible', ',', 'and', 'pray', 'to', 'the', 'same', 'God', ';', 'and', 'each', 'invokes', 'his', 'aid', 'against', 'the', 'other', '.<stop>', 'It', 'may', 'seem', 'strange', 'that', 'any', 'men', 'should', 'dare', 'to', 'ask', 'a', 'just', \"God's\", 'assistance', 'in', 'wringing', 'their', 'bread', 'from', 'the', 'sweat', 'of', 'other', \"men's\", 'faces', ';', 'but', 'let', 'us', 'judge', 'not', ',', 'that', 'we', 'be', 'not', 'judged', '.<stop>', 'The', 'prayers', 'of', 'both', 'could', 'not', 'be', 'answered', '--', 'that', 'of', 'neither', 'has', 'been', 'answered', 'fully', '.<stop>', 'The', 'Almighty', 'has', 'his', 'own', 'purposes', '.<stop>', '\"', 'Woe', 'unto', 'the', 'world', 'because', 'of', 'offenses', '!<stop>', 'for', 'it', 'must', 'needs', 'be', 'that', 'offenses', 'come', ';', 'but', 'woe', 'to', 'that', 'man', 'by', 'whom', 'the', 'offense', 'cometh', '\"', '.<stop>', 'If', 'we', 'shall', 'suppose', 'that', 'American', 'slavery', 'is', 'one', 'of', 'those', 'offenses', 'which', ',', 'in', 'the', 'providence', 'of', 'God', ',', 'must', 'needs', 'come', ',', 'but', 'which', ',', 'having', 'continued', 'through', 'his', 'appointed', 'time', ',', 'he', 'now', 'wills', 'to', 'remove', ',', 'and', 'that', 'he', 'gives', 'to', 'both', 'North', 'and', 'South', 'this', 'terrible', 'war', ',', 'as', 'the', 'woe', 'due', 'to', 'those', 'by', 'whom', 'the', 'offense', 'came', ',', 'shall', 'we', 'discern', 'therein', 'any', 'departure', 'from', 'those', 'divine', 'attributes', 'which', 'the', 'believers', 'in', 'a', 'living', 'God', 'always', 'ascribe', 'to', 'him', '?<stop>', 'Fondly', 'do', 'we', 'hope', '--', 'fervently', 'do', 'we', 'pray', '--', 'that', 'this', 'mighty', 'scourge', 'of', 'war', 'may', 'speedily', 'pass', 'away', '.<stop>', 'Yet', ',', 'if', 'God', 'wills', 'that', 'it', 'continue', 'until', 'all', 'the', 'wealth', 'piled', 'by', 'the', \"bondsman's\", 'two', 'hundred', 'and', 'fifty', 'years', 'of', 'unrequited', 'toil', 'shall', 'be', 'sunk', ',', 'and', 'until', 'every', 'drop', 'of', 'blood', 'drawn', 'by', 'the', 'lash', 'shall', 'be', 'paid', 'by', 'another', 'drawn', 'with', 'the', 'sword', ',', 'as', 'was', 'said', 'three', 'thousand', 'years', 'ago', ',', 'so', 'still', 'it', 'must', 'be', 'said', ',', '\"', 'The', 'judgments', 'of', 'the', 'Lord', 'are', 'true', 'and', 'righteous', 'altogether', '\"', '.<stop>', 'With', 'malice', 'toward', 'none', ';', 'with', 'charity', 'for', 'all', ';', 'with', 'firmness', 'in', 'the', 'right', ',', 'as', 'God', 'gives', 'us', 'to', 'see', 'the', 'right', ',', 'let', 'us', 'strive', 'on', 'to', 'finish', 'the', 'work', 'we', 'are', 'in', ';', 'to', 'bind', 'up', 'the', \"nation's\", 'wounds', ';', 'to', 'care', 'for', 'him', 'who', 'shall', 'have', 'borne', 'the', 'battle', ',', 'and', 'for', 'his', 'widow', ',', 'and', 'his', 'orphan', '--', 'to', 'do', 'all', 'which', 'may', 'achieve', 'and', 'cherish', 'a', 'just', 'and', 'lasting', 'peace', 'among', 'ourselves', ',', 'and', 'with', 'all', 'nations', '.<stop>']\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(r'~/5thsem/NLP/assignments/assignment1')\n",
    "#path = './a.txt'\n",
    "tokenize('./Gutenberg/txt/Ab*.txt',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08q0ospof2z9"
   },
   "outputs": [],
   "source": [
    "def sentence_tokenize(path):\n",
    "    files = glob.glob(path)\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name,'r',errors = 'replace') as f:\n",
    "                text = f.read()\n",
    "                #text = \"I said, 'what're you? Crazy?' said Sandowsky's dog! I can't afford to do that abc-def.\"\n",
    "                text = \" \" + text + \"  \"\n",
    "                text = text.replace(\"\\n\",\" \")# replace newline with space\n",
    "\n",
    "                text = re.sub(prefixes,\"\\\\1<prd>\",text)# replace period in titles with <prd>\n",
    "                text = re.sub(websites,\"<prd>\\\\1\",text)# replace period in websites with <prd>\n",
    "                if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "                text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "                text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)# ex. replace Ok Inc. Mr. Sal with <prd> after  Inc \n",
    "                text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "                text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "                text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "                text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "                text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "                \n",
    "                if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "                if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "                if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "                if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "                text = text.replace(\".\",\".<stop>\")\n",
    "                text = text.replace(\"?\",\"?<stop>\")\n",
    "                text = text.replace(\"!\",\"!<stop>\")\n",
    "                text = text.replace(\"<prd>\",\".\")# putting the full stops back in place\n",
    "                sentences = text.split(\"<stop>\")\n",
    "                sentences = sentences[:-1]\n",
    "                sentences = [s.strip() for s in sentences]\n",
    "                corpus.append(sentences)\n",
    "\n",
    "                f.close()\n",
    "\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "    #print(corpus[0])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Gutenberg/txt/Ab*.txt'\n",
    "sentence_tokenize(path)\n",
    "\n",
    "# sentence = \"I like you\"\n",
    "# sentence_tokenize(sentence,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gwBEe5bgAbn"
   },
   "source": [
    "# Language Model and Smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input):#flattening a 2d list  to a 1d list\n",
    "    words = []\n",
    "    for i in input:\n",
    "        for j in i:\n",
    "            words.append(j)\n",
    "    return words\n",
    "\n",
    "words = flatten(corpus2)\n",
    "#print(words[-2])\n",
    "\n",
    "from collections import Counter # counting frequencies of words\n",
    "exclusion_list = [\",\", \".<stop>\", '\"','-',':',';',\"'\"]\n",
    "Counter = Counter(word.rstrip() for word in words if word not in exclusion_list)\n",
    "most_occur = []\n",
    "most_occur = Counter.most_common(100)\n",
    "print(most_occur)\n",
    "a = ('word','frequency')\n",
    "most_occur_table = [a] + most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as FF\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "data = most_occur_table\n",
    "df = data[0:11]\n",
    "\n",
    "table = FF.create_table(df) # making a table\n",
    "#py.sign_in('abhinalla','••••••••••')\n",
    "iplot(table, filename='word-count-sample')\n",
    "\n",
    "data = most_occur\n",
    "iplot([{\"x\" : list(zip(*data))[0], \"y\": list(zip(*data))[1]}])# plotting of frequencies against words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------------ method 2\n",
    "# Unigram\n",
    "import math\n",
    "\n",
    "def token_counts(corpus):\n",
    "    tokens = {}\n",
    "    #for file in corp:\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            if word.lower() not in exclusion_list:\n",
    "                if word.lower() not in tokens:\n",
    "                    tokens[word.lower()] = 0\n",
    "                tokens[word.lower()] += 1\n",
    "    return tokens\n",
    "# print(corpus2[0])\n",
    "from ipy_table import *\n",
    "unigrams = token_counts(corpus2)\n",
    "#print(unigrams)\n",
    "sorted_unigrams = sorted(unigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "display(make_table(sorted_unigrams[:10]))\n",
    "\n",
    "sorted_unigrams2 = sorted(unigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "display(make_table(sorted_unigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : list(zip(*sorted_unigrams[:1000]))[0], \"y\": list(zip(*sorted_unigrams[:1000]))[1]}])\n",
    "\n",
    "x = list(zip(*sorted_unigrams[:1000]))[0]\n",
    "y = list(zip(*sorted_unigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "#----------------------------------\n",
    "all_sort_uni = sorted_unigrams[:len(unigrams)]\n",
    "\n",
    "freq_class_uni = {}\n",
    "\n",
    "for each in all_sort_uni:\n",
    "#     print(each[1])\n",
    "    if each[1] not in freq_class_uni:\n",
    "        freq_class_uni[each[1]] = 0\n",
    "    freq_class_uni[each[1]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram ------------------\n",
    "def get_bigrams(corpus):\n",
    "    bigrams = {}\n",
    "    for sentence in corpus:\n",
    "        for index, word in enumerate(sentence):\n",
    "            if index > 0:\n",
    "                pair = (sentence[index - 1].lower(), word.lower())\n",
    "                if word.lower() not in exclusion_list and sentence[index-1].lower() not in exclusion_list:\n",
    "                    if pair not in bigrams:\n",
    "                        bigrams[pair] =0\n",
    "                    bigrams[pair] += 1        \n",
    "    return bigrams\n",
    "\n",
    "# Note :: Try this with the smaller corpus first.\n",
    "bigrams = get_bigrams(corpus2)\n",
    "sorted_bigrams = sorted(bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "sorted_bigrams2 = sorted(bigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "\n",
    "sorted_bigrams = sorted_bigrams[:1000] # RAM problems\n",
    "print(sorted_bigrams[:100]) \n",
    "display(make_table(sorted_bigrams[:10]))\n",
    "display(make_table(sorted_bigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : ['_'.join(x) for x in list(zip(*sorted_bigrams))[0]], \"y\": list(zip(*sorted_bigrams))[1]}])\n",
    "\n",
    "x = ['_'.join(x) for x in list(zip(*sorted_bigrams))[0]]\n",
    "y = list(zip(*sorted_bigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "\n",
    "# ----------------------------------------------\\\n",
    "\n",
    "sorted_bigrams = sorted(bigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "all_sort = sorted_bigrams[:len(bigrams)]\n",
    "\n",
    "freq_class_bi = {}\n",
    "\n",
    "for each in all_sort:\n",
    "#     print(each[1])\n",
    "    if each[1] not in freq_class_bi:\n",
    "        freq_class_bi[each[1]] = 0\n",
    "    freq_class_bi[each[1]] += 1\n",
    "#     print(freq_class[each[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram ------------------\n",
    "def get_trigrams(corpus):\n",
    "    trigrams = {}\n",
    "    for sentence in corpus:\n",
    "        for index, word in enumerate(sentence):\n",
    "            if index > 1:\n",
    "                trio = (sentence[index-2].lower(), sentence[index - 1].lower(), word.lower())\n",
    "                if word.lower() not in exclusion_list and sentence[index-2].lower() not in exclusion_list and sentence[index-1].lower() not in exclusion_list:\n",
    "                    if trio not in trigrams:\n",
    "                        trigrams[trio] =0\n",
    "                    trigrams[trio] += 1        \n",
    "    return trigrams\n",
    "\n",
    "# Note :: Try this with the smaller corpus first.\n",
    "trigrams = get_trigrams(corpus2)\n",
    "sorted_trigrams = sorted(trigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "sorted_trigrams2 = sorted(trigrams.items(), key = lambda x: x[1], reverse=False)\n",
    "\n",
    "sorted_trigrams = sorted_trigrams[:100] # RAM problems\n",
    "print(sorted_trigrams) \n",
    "display(make_table(sorted_trigrams[:10]))\n",
    "display(make_table(sorted_trigrams2[:10]))\n",
    "\n",
    "fig1 = iplot([{\"x\" : ['_'.join(x) for x in list(zip(*sorted_trigrams))[0]], \"y\": list(zip(*sorted_trigrams))[1]}])\n",
    "\n",
    "x = ['_'.join(x) for x in list(zip(*sorted_trigrams))[0]]\n",
    "y = list(zip(*sorted_trigrams[:1000]))[1]\n",
    "y = [math.log10(val) for val in y]\n",
    "fig2 = iplot([{\"x\" : x, \"y\": y}])\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=1)\n",
    "\n",
    "#----------------------------------------\n",
    "sorted_trigrams = sorted(trigrams.items(), key = lambda x: x[1], reverse=True)\n",
    "all_sort = sorted_trigrams[:len(trigrams)]\n",
    "\n",
    "freq_class_tri = {}\n",
    "\n",
    "for each in all_sort:\n",
    "    #print(each[1])\n",
    "    if each[1] not in freq_class_tri:\n",
    "        freq_class_tri[each[1]] = 0\n",
    "    freq_class_tri[each[1]] += 1\n",
    "\n",
    "#graphs wont be smooth because we are taking only 100 tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d, reverse = True):\n",
    "    return sorted(d.items(), key = lambda x : x[1], reverse = reverse)\n",
    "\n",
    "def unigram_probs(unigrams):\n",
    "    new_unigrams = {}\n",
    "    N = sum(unigrams.values())\n",
    "    for word in unigrams:\n",
    "        new_unigrams[word] = round(unigrams[word] / float(N), 15)\n",
    " #new_unigrams[word] = math.log(unigrams[word] / float(N))\n",
    "    return new_unigrams\n",
    " \n",
    "uprobs = unigram_probs(unigrams)\n",
    "sorted_uprobs = sort_dict(uprobs)\n",
    "make_table(sorted_uprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probs(unigrams):\n",
    "    new_bigrams = {}\n",
    "    N = sum(bigrams.values())\n",
    "    for word in bigrams:\n",
    "        new_bigrams[word] = round(bigrams[word] / float(N), 15)\n",
    " #new_unigrams[word] = math.log(unigrams[word] / float(N))\n",
    "    return new_bigrams\n",
    " \n",
    "bprobs = bigram_probs(bigrams)\n",
    "sorted_bprobs = sort_dict(bprobs)\n",
    "make_table(sorted_bprobs[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N from (N-1) gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Th9ogrogKTO"
   },
   "outputs": [],
   "source": [
    "#print(corpus2)\n",
    "def complete_ngram(ngram, n):\n",
    "    comp_ngram = {}\n",
    "    for each in corpus2:\n",
    "        for index, word in enumerate(each):\n",
    "            if word not in exclusion_list:\n",
    "                if index + n <= len(each) - 1:\n",
    "                    flag = 0\n",
    "                    for i in range(n):                        \n",
    "                        if ngram[i].lower() != each[index+i].lower():\n",
    "                            flag=1\n",
    "                            break\n",
    "\n",
    "                    if flag == 0:\n",
    "                        list_ng = list(ngram)\n",
    "                        if each[index+n-1].lower() not in exclusion_list:\n",
    "                            list_ng.append(each[index+n].lower())              \n",
    "                            ng_tuple = tuple(list_ng)\n",
    "        #                     list_ngram = list(ngram)\n",
    "        #                     list_ngram.append(each[index+n-1].lower())\n",
    "                            if ng_tuple not in comp_ngram:\n",
    "                                    comp_ngram[ng_tuple] =0\n",
    "                            #print('HERE')\n",
    "                            #print(ng_tuple)\n",
    "                    \n",
    "                            comp_ngram[ng_tuple] += 1\n",
    "                            #print(comp_ngram[ng_tuple])\n",
    "    return sort_dict(comp_ngram)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ngram(('the','united'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def laplace_smoothing(n_grams):\n",
    "    n = len(n_grams)\n",
    " #   print(n)\n",
    "    if n==1:\n",
    "  #      print(n_grams)     \n",
    "        if n_grams not in unigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = unigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(unigrams.values()) + n )* 1.0)  \n",
    "    if n == 2:\n",
    "        if n_grams not in bigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = bigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(bigrams.values()) + n*(n-1)) * 1.0)\n",
    "    if n == 3:\n",
    "        if n not in trigrams:\n",
    "            count_value = 1\n",
    "        else:\n",
    "            count_value = trigrams[n_grams] + 1\n",
    "        ans_value = count_value / ((sum(bigrams.values()) + n*(n-1)*(n-2)) * 1.0)\n",
    "    return ans_value\n",
    "\n",
    "\n",
    "def good_turing(n_grams):\n",
    "    \n",
    "    if len(list(n_grams)) == 3:\n",
    "        if n_grams in trigrams:\n",
    "#             trigrams[word] + 1\n",
    "            if trigrams[n_grams] + 1 in freq_class_tri:\n",
    "                num = (trigrams[n_grams] + 1) * freq_class_tri[trigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_tri[trigrams[n_grams]]\n",
    "            prob = (num/den)/sum(trigrams.values())\n",
    "        else: \n",
    "            count = 1\n",
    "            if count in freq_class_tri:\n",
    "                num = (trigrams[n_grams] + 1) * freq_class_tri[trigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_tri[trigrams[n_grams]]\n",
    "            prob = num/den\n",
    "            \n",
    "    if len(list(n_grams)) == 2:\n",
    "        if n_grams in bigrams:\n",
    "#             trigrams[word] + 1\n",
    "            if bigrams[n_grams] + 1 in freq_class_bi:\n",
    "                num = (bigrams[n_grams] + 1) * freq_class_bi[bigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            print(bigrams[n_grams])\n",
    "            print(freq_class_bi[bigrams[n_grams]])\n",
    "            den = freq_class_bi[bigrams[n_grams]]\n",
    "            \n",
    "            prob = (num/den)/sum(freq_class_bi.values())\n",
    "        else: \n",
    "            count = 1\n",
    "            if count in freq_class_bi:\n",
    "                num = (bigrams[n_grams] + 1) * freq_class_bi[bigrams[n_grams] + 1]\n",
    "            else:\n",
    "                num = 0\n",
    "            den = freq_class_bi[bigrams[n_grams]]\n",
    "            prob = num/den\n",
    "            \n",
    "    return prob\n",
    "\n",
    "def good_turing_uni(n_grams):\n",
    "#     print('HERE')\n",
    "#     print(unigrams)\n",
    "    if n_grams in unigrams:\n",
    "        if unigrams[n_grams] + 1 in freq_class_uni:\n",
    "            prob = ((unigrams[n_grams]+ 1) * freq_class_uni[unigrams[n_grams]] / (freq_class_uni[unigrams[n_grams] ] * 1.0)) / (freq_class_uni[1] * 1.0)\n",
    "        else:\n",
    "            prob = 0\n",
    "    else:\n",
    "        if 1 in freq_class_uni:\n",
    "            prob = ((unigrams[n_grams] + 1) * freq_class_uni[unigrams[n_grams]] / ( sum(unigrams.values()) * 1.0))\n",
    "        else:\n",
    "            prob = 0\n",
    "\n",
    "    return prob\n",
    "\n",
    "def deleted_interpolation(n_grams):\n",
    "  # perform deleted Interpolation\n",
    "#     list_grams = list(n_grams)\n",
    "    n = len(n_grams)\n",
    "    if n == 1:\n",
    "        return laplace_smoothing(n_grams)\n",
    "    if n == 2:\n",
    "        return 0.7*laplace_smoothing(n_grams) + 0.3*laplace_smoothing((n_grams[1],))\n",
    "    else:\n",
    "        return ((0.5)*laplace_smoothing(n_grams) + (0.4)*laplace_smoothing(n_grams[1:3]) + (0.1)*laplace_smoothing((n_grams[2],)))\n",
    "\n",
    "\n",
    "def kneser_ney(n_grams):\n",
    "  # perform Kneser-Ney smoothing\n",
    "    pass\n",
    "\n",
    "\n",
    "# only 1 of the next 2 have to be implemented\n",
    "\n",
    "def backoff(n_grams):\n",
    "  # perform Backoff\n",
    "    pass\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams[('as','soon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth1 = ['of','the']\n",
    "\n",
    "#laplace_smoothing((smooth1[0],smooth1[1]))\n",
    "\n",
    "# good_turing(('as','soon'))\n",
    "#good_turing_uni(('a'))\n",
    "\n",
    "# deleted_interpolation(('the','united','states'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjoF5Wlsaf_M"
   },
   "source": [
    "# Unigrams and Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Check and Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oehobp6dbXp7"
   },
   "outputs": [],
   "source": [
    "alphas = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "def spell_checker(check_word, n_grams):\n",
    "    list_correct = []\n",
    "    max_correct = ''\n",
    "    max_count=-1\n",
    "    word_count = 0\n",
    "    for each_alpha in alphas:\n",
    "        for i in range(len(list(check_word))):\n",
    "#             print(i)\n",
    "            list_word = list(check_word)\n",
    "            list_word[i] = each_alpha\n",
    "#             new_ word = check_word.replace(check_word[i],each_alpha)\n",
    "            if (\"\".join(list_word)) in n_grams:\n",
    "                new_word = \"\".join(list_word)\n",
    "                list_correct.append([new_word,unigrams[new_word]])\n",
    "                if unigrams[new_word] > max_count:\n",
    "                    max_count = unigrams[new_word]\n",
    "                    max_correct = new_word\n",
    "    for each_alpha in alphas:\n",
    "        for i in range(len(check_word) - 1):\n",
    "            list_word = list(check_word)\n",
    "            list_word[i] = each_alpha\n",
    "#             new_word = check_word[i].replace(check_word[i],each_alpha)\n",
    "            for each_next_alpha in alphas:\n",
    "                list_word[i+1] = each_next_alpha\n",
    "#                 new_word = new_word[i].replace(new_word[i+1],each_next_alpha)\n",
    "                if (\"\".join(list_word)) in n_grams:\n",
    "                    new_word = \"\".join(list_word)\n",
    "                    list_correct.append([new_word,unigrams[new_word]])\n",
    "                    if unigrams[new_word] > max_count:\n",
    "                        max_count = unigrams[new_word]\n",
    "                        max_correct = new_word\n",
    "                    \n",
    "    return max_correct, max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker('spalling',unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgPpfeyYgLUv"
   },
   "source": [
    "# Grammaticality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX0RvOpLgYgR"
   },
   "outputs": [],
   "source": [
    "def score_grammaticality(sentence):\n",
    "    words = []\n",
    "    value = 1   \n",
    "    print(sentence)\n",
    "    arr = tokenize(sentence,2)\n",
    "    print(arr)\n",
    "    #re.split(r':|;|,|-| ',sentence)\n",
    "    if arr:       \n",
    "        for each in arr:\n",
    "            print(each)\n",
    "            for word in each:\n",
    "                \n",
    "                word = word.strip(\"'|)|?|!|.|,|(\")\n",
    "                word = word.strip('\"')\n",
    "                #print(word)\n",
    "                if word:\n",
    "                    words.append((word.lower()))\n",
    "                #print(len(words))\n",
    "                if len(words) == 1:\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "                elif len(words) == 2:\n",
    "                    value = value*laplace_smoothing((words[0],words[1]))\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "                elif len(words)>2:\n",
    "                    for j in range(len(words)-2):\n",
    "                        value = value*(laplace_smoothing((words[j],words[j+1],words[j+2])))\n",
    "                    value = value*(laplace_smoothing((words[0],words[1])))\n",
    "                    value = value*laplace_smoothing((words[0],))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = score_grammaticality('I have a red apple.')\n",
    "print(x)\n",
    "\n",
    "g = score_grammaticality('apple a have I red.')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Error Correction Demo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
